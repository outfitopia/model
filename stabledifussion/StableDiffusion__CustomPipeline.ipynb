{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d4cc1430-d020-435b-bc65-9ec54ea1c66a","_uuid":"0f01a3ec-d58c-4978-92f8-8103ea9b5178","trusted":true},"source":["## Text to image using stable diffusion"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"747a3831-8cd5-4541-a14f-4d713306de0c","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"87ce8921-9e93-4b26-9728-bfc7f4a38ba5","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:13:27.697257Z","iopub.status.busy":"2023-08-16T20:13:27.696219Z","iopub.status.idle":"2023-08-16T20:14:05.608899Z","shell.execute_reply":"2023-08-16T20:14:05.607673Z","shell.execute_reply.started":"2023-08-16T20:13:27.697156Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# !pip install diffusers==0.3.0 --q\n","# !pip install transformers scipy ftfy --q\n","# !pip install \"ipywidgets>=7,<8\" --q\n","# import IPython.display"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4bfb1332-6076-41ed-b175-bd236235fb82","_uuid":"b06fa1ed-2cc7-4520-a38f-42b9448bac6c","trusted":true},"source":["## Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"aa48e450-5685-4de4-b1ad-ae405dcae3fc","_kg_hide-input":true,"_uuid":"9016e132-89c3-4dd6-8d7f-0c20dae22334","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:14:36.301628Z","iopub.status.busy":"2023-08-16T20:14:36.300741Z","iopub.status.idle":"2023-08-16T20:14:40.426920Z","shell.execute_reply":"2023-08-16T20:14:40.425694Z","shell.execute_reply.started":"2023-08-16T20:14:36.301577Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import gc\n","import torch\n","from PIL import Image\n","import IPython.display \n","from torch import autocast\n","from tqdm.auto import tqdm\n","# from kaggle_secrets import UserSecretsClient\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import StableDiffusionPipeline\n","from diffusers import AutoencoderKL, UNet2DConditionModel\n","from diffusers import LMSDiscreteScheduler , PNDMScheduler\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","# user_secrets = UserSecretsClient()\n","# Hugging_face  = user_secrets.get_secret(\"Hugging_id\")\n","Hugging_face = \"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\""]},{"cell_type":"markdown","metadata":{"_cell_guid":"20c2bf56-a0bb-413f-ac1f-65b487f6deae","_uuid":"51d1a928-6c69-4635-a32e-545b794fda40","trusted":true},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"0194e0ae-2883-42fb-b35f-53f973556897","_uuid":"d8915287-1ea3-4ed8-a64f-8c8b15c9a604","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:16:58.740094Z","iopub.status.busy":"2023-08-16T20:16:58.739037Z","iopub.status.idle":"2023-08-16T20:16:58.814487Z","shell.execute_reply":"2023-08-16T20:16:58.813409Z","shell.execute_reply.started":"2023-08-16T20:16:58.740051Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class config : \n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    HEIGHT = 512                        \n","    WIDTH = 512                         \n","    NUM_INFERENCE_STEPS = 50            \n","    GUIDANCE_SCALE = 20                \n","    GENERATOR = torch.manual_seed(50)   \n","    BATCH_SIZE = 1"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a9a7861e-2946-40ff-8aff-68405c9edcdc","_uuid":"0e89869a-6b1e-4efb-a758-6e8bb9d41100","trusted":true},"source":["## Helper functions"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"11395b5b-0f66-4ed5-80c7-cba97cff6fbd","_kg_hide-input":true,"_uuid":"328f713f-e231-400a-a6a7-ea1d2431a290","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:17:04.715974Z","iopub.status.busy":"2023-08-16T20:17:04.715596Z","iopub.status.idle":"2023-08-16T20:17:04.723548Z","shell.execute_reply":"2023-08-16T20:17:04.722543Z","shell.execute_reply.started":"2023-08-16T20:17:04.715943Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def image_grid(imgs, rows, cols):\n","    assert len(imgs) == rows*cols\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', size=(cols*w, rows*h))\n","    grid_w, grid_h = grid.size\n","    for i, img in enumerate(imgs):\n","        grid.paste(img, box=(i%cols*w, i//cols*h))\n","    return grid"]},{"cell_type":"markdown","metadata":{"_cell_guid":"32a1e31a-ce57-48ee-b110-c58c5d5fe678","_uuid":"d2d561ba-d014-44e8-9f0f-c533cb5922f3","trusted":true},"source":["## Loading the pretrained models\n","\n","* <font size = 3><span style=\"color:#3A3E59\"> The model we are going to use is `CompVis/stable-diffusion-v1-4` the model card can be found <a href =https://huggingface.co/CompVis/stable-diffusion-v1-4>here</a> </span></font>\n","* <font size = 3><span style=\"color:#3A3E59\"> We are going to load \n","     `Variable auto encoder`,\n","     `Tokenizer`,\n","     `Text encoder`  and\n","     `Unet`</span></font>\n","     \n","* <font size = 3><span style=\"color:#3A3E59\">Stable Diffusion during inference</span></font>"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"5cf24ad6-4248-416c-9af9-f04c730ae168","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"1bb766b2-70b7-46b8-89f0-cb45733e3d69","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:17:10.701554Z","iopub.status.busy":"2023-08-16T20:17:10.700962Z","iopub.status.idle":"2023-08-16T20:20:06.142879Z","shell.execute_reply":"2023-08-16T20:20:06.141804Z","shell.execute_reply.started":"2023-08-16T20:17:10.701516Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b6faf3ab-8b8a-46ad-b108-00db2777c3f3","_kg_hide-input":true,"_uuid":"7c2d1571-5f4e-409f-869c-623945340484","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:20:56.762625Z","iopub.status.busy":"2023-08-16T20:20:56.762217Z","iopub.status.idle":"2023-08-16T20:20:56.768997Z","shell.execute_reply":"2023-08-16T20:20:56.767725Z","shell.execute_reply.started":"2023-08-16T20:20:56.762592Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[94mTokenizer, Text Encoder, VAE, Unet are loaded !!!\n"]}],"source":["unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n","vae = vae.to(config.DEVICE)\n","text_encoder = text_encoder.to(config.DEVICE)\n","unet = unet.to(config.DEVICE)\n","\n","print(f'\\033[94mTokenizer, Text Encoder, VAE, Unet are loaded !!!')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"62ff775c-9383-4635-93f7-3375cffde989","_uuid":"5e50836d-3b72-48ea-9c75-da9cc78eea7c","trusted":true},"source":["## Scheduler\n","\n","<font size = 5><span style=\"color:#F60195\"> </span></font>\n","* <font size = 3><span style=\"color:#3A3E59\"> Using K - LMS Scheduler</span></font>\n","* <font size = 3><span style=\"color:#3A3E59\"> The default scheduler is PNDM scheduler </span></font>\n","* <font size = 3><span style=\"color:#3A3E59\"> Some other schedulers are DDIM  ,DDPM and <a href = https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers> some more </a></span></font>"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"cedf4523-4f8a-4fed-afdb-06ddd8c255f9","_kg_hide-input":true,"_uuid":"89cb4a95-2559-47b2-89c9-419168f413f5","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:21:11.524304Z","iopub.status.busy":"2023-08-16T20:21:11.523902Z","iopub.status.idle":"2023-08-16T20:21:11.533627Z","shell.execute_reply":"2023-08-16T20:21:11.532504Z","shell.execute_reply.started":"2023-08-16T20:21:11.524269Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[94mThe scheduler loaded is K-LMS Sceheduler\n"]}],"source":["scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","print(f'\\033[94mThe scheduler loaded is K-LMS Sceheduler')"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"d221304b-4261-43e8-8271-fec1404494f8","_kg_hide-input":true,"_uuid":"611ba819-9f68-4431-a6db-c9bd5425a859","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:22:02.986923Z","iopub.status.busy":"2023-08-16T20:22:02.986522Z","iopub.status.idle":"2023-08-16T20:22:02.992566Z","shell.execute_reply":"2023-08-16T20:22:02.991240Z","shell.execute_reply.started":"2023-08-16T20:22:02.986891Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["prompt = [\"black basketball nike shoes with blue laces\"]"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"ecb7c79f-ddad-446f-95a6-2e2ff4d47bde","_kg_hide-input":true,"_uuid":"b863cc3f-df28-4a08-9c50-1b2bb949da28","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:22:07.762933Z","iopub.status.busy":"2023-08-16T20:22:07.762538Z","iopub.status.idle":"2023-08-16T20:22:08.826530Z","shell.execute_reply":"2023-08-16T20:22:08.825462Z","shell.execute_reply.started":"2023-08-16T20:22:07.762901Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[94mText Embeddings shape: torch.Size([2, 77, 768])\n"]}],"source":["text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n","max_length = text_input.input_ids.shape[-1]\n","with torch.no_grad():\n","      text_embeddings = text_encoder(text_input.input_ids.to(config.DEVICE))[0]\n","uncond_input = tokenizer(\n","    [\"\"] * config.BATCH_SIZE, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",")\n","with torch.no_grad():\n","      uncond_embeddings = text_encoder(uncond_input.input_ids.to(config.DEVICE))[0]   \n","text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n","print(f'\\033[94mText Embeddings shape: {text_embeddings.shape}')"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b93052f5-9d4f-4bb6-8ca5-d5ebf3fec062","_kg_hide-input":true,"_uuid":"6b81712a-564b-4a74-a874-3dd8e52ccc6a","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:22:39.113355Z","iopub.status.busy":"2023-08-16T20:22:39.112972Z","iopub.status.idle":"2023-08-16T20:22:39.121067Z","shell.execute_reply":"2023-08-16T20:22:39.119981Z","shell.execute_reply.started":"2023-08-16T20:22:39.113323Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[94mLatent shape: torch.Size([1, 4, 64, 64])\n"]}],"source":["latents = torch.randn(\n","  (config.BATCH_SIZE, unet.in_channels, config.HEIGHT // 8, config.WIDTH // 8),\n","  generator=config.GENERATOR,\n",")\n","latents = latents.to(config.DEVICE)\n","\n","print(f'\\033[94mLatent shape: {latents.shape}')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"83bcbb25-925c-4fb5-8d00-bdbc6d1249be","_uuid":"50493402-d5ff-4831-96ab-5d56fdacca86","trusted":true},"source":["## Encoding the image"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"8502747a-bffb-45ff-b25a-da0618b93a1b","_kg_hide-input":true,"_uuid":"0b07dd21-a371-49c5-9abc-490e349ae515","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:22:44.437614Z","iopub.status.busy":"2023-08-16T20:22:44.437213Z","iopub.status.idle":"2023-08-16T20:22:44.481555Z","shell.execute_reply":"2023-08-16T20:22:44.480652Z","shell.execute_reply.started":"2023-08-16T20:22:44.437582Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["scheduler.set_timesteps(config.NUM_INFERENCE_STEPS)\n","latents = latents * scheduler.sigmas[0]"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"de3efc92-76db-4eba-b557-e51c2495cebb","_kg_hide-input":true,"_uuid":"0552b13f-420f-44aa-8731-65b323527785","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:22:48.225467Z","iopub.status.busy":"2023-08-16T20:22:48.224900Z","iopub.status.idle":"2023-08-16T20:25:52.106390Z","shell.execute_reply":"2023-08-16T20:25:52.105188Z","shell.execute_reply.started":"2023-08-16T20:22:48.225416Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d677b2b36f294c198041cb5e280c997b","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m latent_model_input \u001b[38;5;241m/\u001b[39m ((sigma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m       noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     11\u001b[0m noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m noise_pred_uncond \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mGUIDANCE_SCALE \u001b[38;5;241m*\u001b[39m (noise_pred_text \u001b[38;5;241m-\u001b[39m noise_pred_uncond)\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/unet_2d_condition.py:970\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m     upsample_size \u001b[39m=\u001b[39m down_block_res_samples[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(upsample_block, \u001b[39m\"\u001b[39m\u001b[39mhas_cross_attention\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m upsample_block\u001b[39m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 970\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    971\u001b[0m         hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    972\u001b[0m         temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m    973\u001b[0m         res_hidden_states_tuple\u001b[39m=\u001b[39;49mres_samples,\n\u001b[1;32m    974\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    975\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    976\u001b[0m         upsample_size\u001b[39m=\u001b[39;49mupsample_size,\n\u001b[1;32m    977\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    978\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    982\u001b[0m         hidden_states\u001b[39m=\u001b[39msample, temb\u001b[39m=\u001b[39memb, res_hidden_states_tuple\u001b[39m=\u001b[39mres_samples, upsample_size\u001b[39m=\u001b[39mupsample_size\n\u001b[1;32m    983\u001b[0m     )\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/unet_2d_blocks.py:2134\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2133\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m-> 2134\u001b[0m         hidden_states \u001b[39m=\u001b[39m attn(\n\u001b[1;32m   2135\u001b[0m             hidden_states,\n\u001b[1;32m   2136\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   2137\u001b[0m             cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m   2138\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   2139\u001b[0m             encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   2140\u001b[0m             return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2141\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2144\u001b[0m     \u001b[39mfor\u001b[39;00m upsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers:\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/transformer_2d.py:292\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m# 2. Blocks\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 292\u001b[0m     hidden_states \u001b[39m=\u001b[39m block(\n\u001b[1;32m    293\u001b[0m         hidden_states,\n\u001b[1;32m    294\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    296\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    297\u001b[0m         timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    298\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    299\u001b[0m         class_labels\u001b[39m=\u001b[39;49mclass_labels,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[39m# 3. Output\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_input_continuous:\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention.py:155\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    151\u001b[0m     norm_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(hidden_states)\n\u001b[1;32m    153\u001b[0m cross_attention_kwargs \u001b[39m=\u001b[39m cross_attention_kwargs \u001b[39mif\u001b[39;00m cross_attention_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m--> 155\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn1(\n\u001b[1;32m    156\u001b[0m     norm_hidden_states,\n\u001b[1;32m    157\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monly_cross_attention \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    158\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ada_layer_norm_zero:\n\u001b[1;32m    162\u001b[0m     attn_output \u001b[39m=\u001b[39m gate_msa\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m attn_output\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention_processor.py:322\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcross_attention_kwargs):\n\u001b[1;32m    319\u001b[0m     \u001b[39m# The `Attention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessor(\n\u001b[1;32m    323\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    324\u001b[0m         hidden_states,\n\u001b[1;32m    325\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    326\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    327\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    328\u001b[0m     )\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention_processor.py:1117\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb)\u001b[0m\n\u001b[1;32m   1113\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, attn\u001b[39m.\u001b[39mheads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[39m# TODO: add support for attn.scale when we move to Torch 2.1\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m hidden_states \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mscaled_dot_product_attention(\n\u001b[1;32m   1118\u001b[0m     query, key, value, attn_mask\u001b[39m=\u001b[39;49mattention_mask, dropout_p\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, is_causal\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1119\u001b[0m )\n\u001b[1;32m   1121\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, attn\u001b[39m.\u001b[39mheads \u001b[39m*\u001b[39m head_dim)\n\u001b[1;32m   1122\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(query\u001b[39m.\u001b[39mdtype)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","with autocast(config.DEVICE):\n","      for i, t in tqdm(enumerate(scheduler.timesteps)):\n","            \n","            latent_model_input = torch.cat([latents] * 2)\n","            sigma = scheduler.sigmas[i]\n","            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n","\n","            with torch.no_grad():\n","                  noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","\n","            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","            noise_pred = noise_pred_uncond + config.GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)\n","\n","            latents = scheduler.step(noise_pred, t, latents).prev_sample"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b2a3dca6-e97f-4dba-83aa-599fbb6dffb6","_uuid":"bbba6e0d-3884-4d75-b2f7-6b29c771c95a","trusted":true},"source":["## Decoding the image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b911a731-4324-43ed-b4ed-d68e4618c449","_kg_hide-input":true,"_uuid":"c9ff1361-f54f-4d40-aeb2-da7173d89032","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:26:00.194624Z","iopub.status.busy":"2023-08-16T20:26:00.194229Z","iopub.status.idle":"2023-08-16T20:26:00.217486Z","shell.execute_reply":"2023-08-16T20:26:00.216433Z","shell.execute_reply.started":"2023-08-16T20:26:00.194591Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["latents = 1 / 0.18215 * latents\n","\n","with torch.no_grad():\n","  image = vae.decode(latents).sample\n","print(f'\\033[94mImage shape: {image.shape}')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"17177ec8-a421-4a31-9c56-a6d132e0da1a","_uuid":"c07aafa2-6ada-48aa-a290-b7a505d8cd3d","trusted":true},"source":["## Visualizing the image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3de59cd-a5cd-4495-aa7c-cd3411249a97","_kg_hide-input":true,"_uuid":"60192664-332d-45ab-943d-b44f5b956db3","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:26:04.524790Z","iopub.status.busy":"2023-08-16T20:26:04.524390Z","iopub.status.idle":"2023-08-16T20:26:04.683061Z","shell.execute_reply":"2023-08-16T20:26:04.682129Z","shell.execute_reply.started":"2023-08-16T20:26:04.524758Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["image = (image / 2 + 0.5).clamp(0, 1)\n","image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","images = (image * 255).round().astype(\"uint8\")\n","pil_images = [Image.fromarray(image) for image in images]\n","pil_images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7b7c2ce-aa8e-483c-bbdd-87cc8865092c","_kg_hide-input":true,"_uuid":"3f0157af-9efa-4e3e-a6d0-8f2651b9c2d5","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:26:49.468559Z","iopub.status.busy":"2023-08-16T20:26:49.468136Z","iopub.status.idle":"2023-08-16T20:26:49.473738Z","shell.execute_reply":"2023-08-16T20:26:49.472513Z","shell.execute_reply.started":"2023-08-16T20:26:49.468526Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["prompt = [\"extra large size blue kurta with red buttons\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"888b5ccc-c019-47b5-b8f7-b644a12dda56","_kg_hide-input":true,"_uuid":"b2fa99a9-d387-489a-9a09-6f654ee779f4","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:26:53.743520Z","iopub.status.busy":"2023-08-16T20:26:53.742849Z","iopub.status.idle":"2023-08-16T20:26:53.784899Z","shell.execute_reply":"2023-08-16T20:26:53.783842Z","shell.execute_reply.started":"2023-08-16T20:26:53.743428Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n","max_length = text_input.input_ids.shape[-1]\n","with torch.no_grad():\n","      text_embeddings = text_encoder(text_input.input_ids.to(config.DEVICE))[0]\n","uncond_input = tokenizer(\n","    [\"\"] * config.BATCH_SIZE, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",")\n","with torch.no_grad():\n","      uncond_embeddings = text_encoder(uncond_input.input_ids.to(config.DEVICE))[0]   \n","text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n","print(f'\\033[94mText Embeddings shape: {text_embeddings.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ba31d25-e39e-4e75-aad0-d6c7b54866eb","_kg_hide-input":true,"_uuid":"a855d83a-18c2-424e-9a35-7c2e128d75ec","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:26:56.892300Z","iopub.status.busy":"2023-08-16T20:26:56.891294Z","iopub.status.idle":"2023-08-16T20:26:56.899773Z","shell.execute_reply":"2023-08-16T20:26:56.898527Z","shell.execute_reply.started":"2023-08-16T20:26:56.892261Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["latents = torch.randn(\n","  (config.BATCH_SIZE, unet.in_channels, config.HEIGHT // 8, config.WIDTH // 8),\n","  generator=config.GENERATOR,\n",")\n","latents = latents.to(config.DEVICE)\n","\n","print(f'\\033[94mLatent shape: {latents.shape}')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"389b993c-16d0-4d3d-9003-dc3d64c40994","_uuid":"869ba3ac-0f6a-41c5-8194-b18d87f8ae3c","trusted":true},"source":["## Encoding another image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff078191-3d6c-4409-b9bf-ca201c569330","_kg_hide-input":true,"_uuid":"4a682761-512c-4ff1-82d2-809aa84d67d1","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:26:59.782398Z","iopub.status.busy":"2023-08-16T20:26:59.781981Z","iopub.status.idle":"2023-08-16T20:26:59.789596Z","shell.execute_reply":"2023-08-16T20:26:59.788276Z","shell.execute_reply.started":"2023-08-16T20:26:59.782361Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["scheduler.set_timesteps(config.NUM_INFERENCE_STEPS)\n","latents = latents * scheduler.sigmas[0]"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"76b4ab27-2aea-40e2-963e-b0aa0e73f3f2","_kg_hide-input":true,"_uuid":"abad16b8-d192-4898-885f-e89194c264b6","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:27:03.780830Z","iopub.status.busy":"2023-08-16T20:27:03.780418Z","iopub.status.idle":"2023-08-16T20:30:02.471677Z","shell.execute_reply":"2023-08-16T20:30:02.470498Z","shell.execute_reply.started":"2023-08-16T20:27:03.780798Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'autocast' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mautocast\u001b[49m(config\u001b[38;5;241m.\u001b[39mDEVICE):\n\u001b[1;32m      2\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(scheduler\u001b[38;5;241m.\u001b[39mtimesteps)):\n\u001b[1;32m      4\u001b[0m         latent_model_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'autocast' is not defined"]}],"source":["\n","with autocast(config.DEVICE):\n","      for i, t in tqdm(enumerate(scheduler.timesteps)):\n","        \n","        latent_model_input = torch.cat([latents] * 2)\n","        sigma = scheduler.sigmas[i]\n","        latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n","\n","        with torch.no_grad():\n","              noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","\n","        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","        noise_pred = noise_pred_uncond + config.GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)\n","\n","        latents = scheduler.step(noise_pred, i, latents).prev_sample"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0582da6b-a5e9-4f27-8303-84a00f9010ce","_uuid":"fea6599d-f73d-4c74-b6a5-d348f6749294","trusted":true},"source":["## Decoding it"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a230127-f4b5-4cbf-a82b-d2a01f707dd4","_kg_hide-input":true,"_uuid":"fae62b11-0764-4240-9e8e-a144ad894559","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:30:13.886013Z","iopub.status.busy":"2023-08-16T20:30:13.885632Z","iopub.status.idle":"2023-08-16T20:30:13.903148Z","shell.execute_reply":"2023-08-16T20:30:13.902130Z","shell.execute_reply.started":"2023-08-16T20:30:13.885981Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["latents = 1 / 0.18215 * latents\n","\n","with torch.no_grad():\n","  image = vae.decode(latents).sample\n","print(f'\\033[94mImage shape: {image.shape}')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"158a60a9-53b8-4e06-80f2-9cb6fd1fe8e4","_uuid":"01650114-4a2c-486a-bb12-b9e2f9e47896","trusted":true},"source":["## Visualizing the image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87bf5725-29b5-422e-a4c3-9027617092fe","_kg_hide-input":true,"_uuid":"2908659e-5982-4090-bc31-f491c99fdd9e","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:30:19.396970Z","iopub.status.busy":"2023-08-16T20:30:19.396176Z","iopub.status.idle":"2023-08-16T20:30:19.543024Z","shell.execute_reply":"2023-08-16T20:30:19.541098Z","shell.execute_reply.started":"2023-08-16T20:30:19.396920Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["image = (image / 2 + 0.5).clamp(0, 1)\n","image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","images = (image * 255).round().astype(\"uint8\")\n","pil_images = [Image.fromarray(image) for image in images]\n","pil_images[0].save(\"img2.jpg\")\n","pil_images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fe6e537-2f7e-4a09-bebb-98bd706aff55","_kg_hide-input":true,"_uuid":"2c1a2162-1dd4-4b41-9a0b-b0239d6fb8a1","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:30:39.991091Z","iopub.status.busy":"2023-08-16T20:30:39.990715Z","iopub.status.idle":"2023-08-16T20:30:40.222336Z","shell.execute_reply":"2023-08-16T20:30:40.221371Z","shell.execute_reply.started":"2023-08-16T20:30:39.991061Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["del latents\n","del vae\n","del text_encoder\n","del unet\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e4040337-4438-4e61-b4b8-51f665ae6758","_uuid":"aa82fc46-2b5e-4985-a974-c51ee65e7745","trusted":true},"source":["## PRETRAINED PIPELINE FOR STABLE DIFFUSION"]},{"cell_type":"markdown","metadata":{"_cell_guid":"eed2e9f9-4450-4512-961e-350b6c119390","_uuid":"2773b8dd-41cf-4a1a-b2a8-c35a79646fe0","trusted":true},"source":["* <font size = 3><span style=\"color:#3A3E59\">StableDiffusionPipeline is an end-to-end inference pipeline that we can use to generate images from text with just a few lines of code.\n","</span></font>"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee014743-17f8-4bbc-89de-251ca4f362b1","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"c2342dd4-394a-4341-af27-d2d0b8a6d96c","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:30:43.279368Z","iopub.status.busy":"2023-08-16T20:30:43.278983Z","iopub.status.idle":"2023-08-16T20:32:23.176274Z","shell.execute_reply":"2023-08-16T20:32:23.174269Z","shell.execute_reply.started":"2023-08-16T20:30:43.279334Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=Hugging_face)  \n","pipe = pipe.to(config.DEVICE)\n","print(f'\\033[94mStable Diffusion Pipeline created !!!')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ba5eba07-ac5f-43e5-876f-a0c5e05056ef","_uuid":"6d5a8e07-0af0-408d-9e57-edbdcf23f8e2","trusted":true},"source":["## Visualizing the images"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5de8f5e9-1c5e-4df6-bc68-4fba0616927b","_uuid":"a56f2076-1413-42f8-be23-1c144c231f65","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T20:36:48.274477Z","iopub.status.busy":"2023-08-16T20:36:48.274060Z","iopub.status.idle":"2023-08-16T20:37:57.143757Z","shell.execute_reply":"2023-08-16T20:37:57.142529Z","shell.execute_reply.started":"2023-08-16T20:36:48.274422Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["num_images = 2\n","prompt = [\"met gala dress, modern, traditional indian, chickenkari, negative: salwar kamiz, fusion saree and gown\"] * num_images\n","with autocast(\"cuda\"):\n","  images = pipe(prompt , num_inference_steps=100).images\n","\n","grid = image_grid(images, rows=1, cols=2)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab9f4af9-1438-4d15-bdf5-ad395a759055","_kg_hide-input":true,"_uuid":"81781935-a5af-4265-af2a-b69e5f09ded7","collapsed":false,"execution":{"iopub.execute_input":"2023-08-16T13:17:47.822951Z","iopub.status.busy":"2023-08-16T13:17:47.822159Z","iopub.status.idle":"2023-08-16T13:20:03.640691Z","shell.execute_reply":"2023-08-16T13:20:03.639451Z","shell.execute_reply.started":"2023-08-16T13:17:47.822912Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["num_images = 2\n","prompt =[\"winter attire: black punk-style outfit, layered with a blue t-shirt. full size view. vibrant yellow shoes.\"] * num_images\n","with autocast(\"cuda\"):\n","  images = pipe(prompt , num_inference_steps=200).images\n","\n","grid = image_grid(images, rows=1, cols=2)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0e813ee-1186-433b-b1d8-433878c97a9c","_kg_hide-input":true,"_uuid":"87672a88-9fde-49d9-b5e7-c83eeaccba30","collapsed":false,"execution":{"iopub.status.busy":"2023-08-16T13:10:33.133622Z","iopub.status.idle":"2023-08-16T13:10:33.135061Z","shell.execute_reply":"2023-08-16T13:10:33.134779Z","shell.execute_reply.started":"2023-08-16T13:10:33.134750Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# num_images = 4\n","# prompt =[\"Cybernetic cloaked anime character concept design, dynamic pose, fantasy anime, dark, bejewelled and encrusted technological royal cloak, powerful aggressive sword stance, biological human face, iridescent, dark and intricate, Greg Rutkowski, Makoto Shinkai, anime CGI, animated, animation, artgerm, artstation, digital illustration, 8k\"] * num_images\n","# with autocast(\"cuda\"):\n","#   images = pipe(prompt , num_inference_steps=200).images\n","\n","# grid = image_grid(images, rows=2, cols=2)\n","# grid"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c956e43-476c-4347-960b-a1e284aba672","_uuid":"8df0b514-3874-42bb-9cde-3b52a817ed0e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
